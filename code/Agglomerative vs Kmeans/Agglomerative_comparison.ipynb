{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0134746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "import time\n",
    "import json\n",
    "from scipy.cluster import hierarchy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88e2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_list(list_of_list):\n",
    "    list_final = []\n",
    "    for i in list_of_list:\n",
    "        list_final += i    \n",
    "    return list(set(list_final))\n",
    "\n",
    "\n",
    "def density_calc(list_base, list_point):\n",
    "    n = len(list_point)\n",
    "    list_base = list(set(list_base))\n",
    "    list_density =[]\n",
    "    list_point_final = []\n",
    "    for i in list_base:\n",
    "        if i in list_point:\n",
    "            list_density.append(list_point.count(i)/n)\n",
    "            list_point_final.append(i)\n",
    "        else:\n",
    "            list_density.append(0) #changed\n",
    "    return  list_point_final, list_density\n",
    "\n",
    "\n",
    "\n",
    "def density_calc_list(list_of_list, list_base):\n",
    "    list_density = []\n",
    "    for i in list_of_list:\n",
    "        list_density.append(np.array([density_calc(list_base, i)[1]]).transpose())\n",
    "    return list_density\n",
    "\n",
    "\n",
    "def create_blank_dataset_with_metadata(m):\n",
    "    data = {\n",
    "        'system num': [],\n",
    "        'data points': [],\n",
    "    }\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        data[f'{i-1}'] = []\n",
    "    data[f'label'] = []\n",
    "    blank_dataset = pd.DataFrame(data)\n",
    "    \n",
    "    return blank_dataset\n",
    "\n",
    "\n",
    "def fill_dataset_with_records(dataset, records):\n",
    "    for record in records:\n",
    "        dataset = pd.concat([dataset, pd.DataFrame([record])], ignore_index=True)\n",
    "    return dataset\n",
    "\n",
    "def make_record(list_of_list, list_p):\n",
    "    records_to_be_added = []\n",
    "    for i in range(len(list_of_list)):\n",
    "        records_to_be_added.append({'system num': i, 'data points': list_of_list[i], 'p':list_p[i]})\n",
    "        \n",
    "    return records_to_be_added\n",
    "\n",
    "\n",
    "def fill_ot_distance(df, Xi, cost_matrix, num_of_iterations, lambda_pen):\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i):\n",
    "            \n",
    "            OT_plan_test = ot.bregman.sinkhorn(df['p'][i].transpose().tolist()[0], df['p'][j].transpose().tolist()[0], cost_matrix, lambda_pen, method='sinkhorn', numItermax=num_of_iterations, stopThr=1e-09, verbose=False, log=False, warn=True, warmstart=None)\n",
    "            OT_cost_test = np.multiply(OT_plan_test, cost_matrix).sum() - lambda_pen*entropy(OT_plan_test)\n",
    "            df[str(i)][j] = OT_cost_test\n",
    "            \n",
    "            \n",
    "def calc_barycenter(df, cost_matrix, lambda_value):\n",
    "    list_p_cluster =  [i.transpose()[0] for i in df['p']]\n",
    "    matrix_p = np.column_stack(list_p_cluster)\n",
    "    return ot.barycenter(matrix_p, cost_matrix, lambda_value, weights=None, method='sinkhorn', numItermax=200, stopThr=0.0001, verbose=False, log=False, warn=True)\n",
    "\n",
    "\n",
    "def condensed_creator(arr):\n",
    "    m = arr.shape[0]\n",
    "\n",
    "    # Extract upper triangle indices\n",
    "    upper_triangle_indices = np.triu_indices(m, k=1)\n",
    "\n",
    "    # Use the indices to get the upper triangle elements\n",
    "    upper_triangle_elements = arr[upper_triangle_indices]\n",
    "\n",
    "    # Convert the elements to a list if needed\n",
    "    upper_triangle_list = upper_triangle_elements.tolist()\n",
    "\n",
    "    # Print or use the resulting list as needed\n",
    "    return upper_triangle_list\n",
    "\n",
    "def plot_dendrogram(df, save_file=False):\n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    matrix_final = matrix + matrix.transpose()\n",
    "    \n",
    "    scaled_matrix = matrix_final\n",
    "    np.fill_diagonal(scaled_matrix, 0)\n",
    "    \n",
    "    matrix_final = condensed_creator(scaled_matrix)\n",
    "    linkage_matrix = linkage(matrix_final, method='complete')\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(linkage_matrix, color_threshold=-np.inf, above_threshold_color='gray')\n",
    "    plt.xlabel('Systems')  # Set the x-axis label to 'System'\n",
    "    plt.xticks([])  # Remove x-axis tick labels\n",
    "    plt.ylabel('Distance')\n",
    "    if save_file:\n",
    "        plt.savefig('../../data/plot_dendrogram.png', format='png', dpi=1000)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def silhouette_score_agglomerative(df):\n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    matrix_final = matrix + matrix.transpose()\n",
    "    min_val = np.min(matrix)\n",
    "    max_val = np.max(matrix)\n",
    "    scaled_matrix = (matrix_final - min_val)\n",
    "    np.fill_diagonal(scaled_matrix, 0)\n",
    "    silhouette_score_list = []\n",
    "    for i in range(2, len(df)):\n",
    "        index_list = cluster_list_creator(df, i)\n",
    "        silhouette_score_list.append(silhouette_score(scaled_matrix, index_list, metric='precomputed'))\n",
    "    return silhouette_score_list\n",
    "\n",
    "def entropy(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    non_zero_entries = matrix[matrix > 0]\n",
    "    entropy_value = -np.sum(non_zero_entries * np.log(non_zero_entries))\n",
    "\n",
    "    return entropy_value\n",
    "\n",
    "def cluster_list_creator(df, num_of_clusters):\n",
    "    \n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    matrix_final = matrix + matrix.transpose()\n",
    "    \n",
    "    min_val = np.min(matrix)\n",
    "    max_val = np.max(matrix)\n",
    "    scaled_matrix = matrix_final\n",
    "    np.fill_diagonal(scaled_matrix, 0)\n",
    "    matrix_final = condensed_creator(scaled_matrix)\n",
    "\n",
    "    linkage_matrix = linkage(matrix_final, method='complete')\n",
    "    \n",
    "    \n",
    "    height = np.shape(linkage_matrix)[0]\n",
    "    list_linkage = [[i] for i in range(len(df))]\n",
    "    for i in range(height):\n",
    "        list_linkage.append(list_linkage[int(linkage_matrix[i][0])] + list_linkage[int(linkage_matrix[i][1])])\n",
    "        \n",
    "        \n",
    "    \n",
    "    list_linkage_inverse = list_linkage[::-1]\n",
    "    list_final = list_linkage_inverse[num_of_clusters-1:]\n",
    "    list_index = []\n",
    "    for i in range(len(df)):\n",
    "        for j in list_final:\n",
    "            if i in j:\n",
    "                list_index.append(list_final.index(j))\n",
    "                break\n",
    "\n",
    "    return list_index\n",
    "\n",
    "def calculate_OT_cost(p, q, reg, cost_matrix, num_iterations, stop_theshold):\n",
    "    p = np.array([p]).T\n",
    "    q = np.array([q]).T\n",
    "    Xi = np.exp(-cost_matrix / reg)\n",
    "    v_n = np.ones((Xi.shape[1], 1))\n",
    "    v_old = v_n\n",
    "    for _ in range(num_iterations):\n",
    "        v_n = q / (Xi.T @ (p / (Xi @ v_n)))\n",
    "        if np.linalg.norm(v_n  - v_old)<stop_theshold:\n",
    "            break\n",
    "        v_old = v_n\n",
    "    diag_u = np.diagflat((p / (Xi @ v_n)))\n",
    "    diag_v = np.diagflat(v_n)\n",
    "    OT_plan = diag_u @ Xi @ diag_v\n",
    "    OT_cost = np.multiply(OT_plan, cost_matrix).sum()\n",
    "    return OT_plan\n",
    "\n",
    "\n",
    "def fill_ot_distance(df, num_of_iterations, lambda_pen, stop_theshold):\n",
    "    for i in range(len(df)):# Here we iterate among rows, and below we shall calculate the densities\n",
    "        for j in range(i+1):\n",
    "\n",
    "            cost_matrix = distance.cdist(df['data points'][i], df['data points'][j])\n",
    "            min_time = time.time()\n",
    "            OT_plan_test = calculate_OT_cost(df['p'][i], df['p'][j], lambda_pen, cost_matrix, num_of_iterations, stop_theshold)            \n",
    "            OT_cost_test = np.multiply(OT_plan_test, cost_matrix).sum()  #yakhoda\n",
    "            max_time = time.time()\n",
    "            df.at[j, str(i)] = OT_cost_test\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def normalize_tuples(list_of_lists):\n",
    "    num_dimensions = len(list_of_lists[0][0])  # Get the number of dimensions from the first tuple\n",
    "    \n",
    "    # Extract all values for each dimension\n",
    "    all_values = [[] for _ in range(num_dimensions)]\n",
    "    for sublist in list_of_lists:\n",
    "        for i, t in enumerate(sublist):\n",
    "            for j in range(num_dimensions):\n",
    "                all_values[j].append(t[j])\n",
    "    \n",
    "    # Compute the minimum and maximum values for each dimension\n",
    "    min_values = [min(dim_values) for dim_values in all_values]\n",
    "    max_values = [max(dim_values) for dim_values in all_values]\n",
    "    \n",
    "    # Normalize each dimension of each tuple\n",
    "    normalized_list_of_lists = []\n",
    "    for sublist in list_of_lists:\n",
    "        normalized_sublist = []\n",
    "        for t in sublist:\n",
    "            normalized_t = tuple((t[j] - min_values[j]) / (max_values[j] - min_values[j]) for j in range(num_dimensions))\n",
    "            normalized_sublist.append(normalized_t)\n",
    "        normalized_list_of_lists.append(normalized_sublist)\n",
    "    \n",
    "    return normalized_list_of_lists\n",
    "\n",
    "\n",
    "def list_of_lists_to_json(list_of_lists):\n",
    "    json_data = {}\n",
    "\n",
    "    for idx, lst in enumerate(list_of_lists):\n",
    "        json_data[idx + 1] = {\n",
    "            'id': idx + 1,\n",
    "            'data points': lst\n",
    "        }\n",
    "    \n",
    "    json_output = json.dumps(json_data, indent=4)\n",
    "    \n",
    "    return json_output\n",
    "def json_content_to_list_of_lists(json_content):\n",
    "    json_data = json.loads(json_content)\n",
    "    \n",
    "    list_of_lists = []\n",
    "    for key in sorted(json_data.keys(), key=int):\n",
    "        list_of_lists.append([tuple(item) for item in json_data[key]['data points']])\n",
    "    \n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92edae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_array(*lst):\n",
    "    r\"\"\" Convert a list if in numpy format \"\"\"\n",
    "    lst_not_empty = [a for a in lst if len(a) > 0 and not isinstance(a, list)]\n",
    "\n",
    "    if len(lst_not_empty) == 0:\n",
    "        type_as = np.zeros(0)\n",
    "\n",
    "    else:\n",
    "        type_as = lst_not_empty[0]\n",
    "    if len(lst) > 1:\n",
    "        return [np.from_numpy(np.array(a), type_as=type_as)\n",
    "                if isinstance(a, list) else a for a in lst]\n",
    "    else:\n",
    "        if isinstance(lst[0], list):\n",
    "            return np.from_numpy(np.array(lst[0]), type_as=type_as)\n",
    "        else:\n",
    "            return lst[0]\n",
    "        \n",
    "        \n",
    "def geometricBar(weights, alldistribT):\n",
    "    \"\"\"return the weighted geometric mean of distributions\"\"\"\n",
    "    weights, alldistribT = list_to_array(weights, alldistribT)\n",
    "    assert (len(weights) == alldistribT.shape[1])\n",
    "    return np.exp(np.dot(np.log(alldistribT), weights.T))\n",
    "\n",
    "\n",
    "def geometricMean(alldistribT):\n",
    "    \"\"\"return the  geometric mean of distributions\"\"\"\n",
    "    alldistribT = list_to_array(alldistribT)\n",
    "    return np.exp(np.mean(np.log(alldistribT), axis=1))\n",
    "\n",
    "\n",
    "def barycenter_sinkhorn(A, M, reg, weights=None, numItermax=1000,\n",
    "                        stopThr=1e-4, verbose=False, log=False, warn=True):\n",
    "    A, M = list_to_array(A, M)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones((A.shape[1],), dtype=A.dtype) / A.shape[1]\n",
    "    else:\n",
    "        assert (len(weights) == A.shape[1])\n",
    "\n",
    "    if log:\n",
    "        log_dict = {'err': []}\n",
    "\n",
    "    K = np.exp(-M / reg)\n",
    "\n",
    "    err = 1\n",
    "\n",
    "    UKv = np.dot(K, (A.T / np.sum(K, axis=0)).T)\n",
    "    u = (geometricMean(UKv) / UKv.T).T\n",
    "    for ii in range(numItermax):\n",
    "\n",
    "        v =  (A / np.dot(K, u))\n",
    "        ps = np.tile(geometricBar(weights, UKv), (A.shape[1], 1)).T\n",
    "        UKv = u * np.dot(K.T, v)\n",
    "        u = ps/np.dot(K, v)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "        if ii % 10 == 1:\n",
    "            err = np.sum(np.std(UKv, axis=1))\n",
    "\n",
    "            if log:\n",
    "                log_dict['err'].append(err)\n",
    "\n",
    "            if err < stopThr:\n",
    "                break\n",
    "            if verbose:\n",
    "                if ii % 200 == 0:\n",
    "                    print('{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n",
    "                print('{:5d}|{:8e}|'.format(ii, err))\n",
    "    else:\n",
    "        if warn:\n",
    "            warnings.warn(\"Sinkhorn did not converge. You might want to \"\n",
    "                          \"increase the number of iterations `numItermax` \"\n",
    "                          \"or the regularization parameter `reg`.\")\n",
    "    if log:\n",
    "        log_dict['niter'] = ii\n",
    "        return geometricBar(weights, UKv), log_dict\n",
    "    else:\n",
    "        return geometricBar(weights, UKv)\n",
    "    \n",
    "    \n",
    "def calc_barycenter(df, cost_matrix, lambda_value):\n",
    "    list_p_cluster =  [i for i in df['p']]\n",
    "    matrix_p = np.column_stack(list_p_cluster)\n",
    "    return barycenter_sinkhorn(matrix_p, cost_matrix, lambda_value, weights=None, numItermax=200, stopThr=0.0001, verbose=False, log=False, warn=True)\n",
    "\n",
    "\n",
    "def dataframe_to_json(df, columns=None):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a JSON format where each row index maps to a dictionary \n",
    "    of column names and their corresponding values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON string representing the DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to dictionary format\n",
    "    if columns==None:\n",
    "        df_dict = df.to_dict(orient='index')\n",
    "    else:\n",
    "        df_dict = df[columns].to_dict(orient='index')\n",
    "    for row in df_dict.values():\n",
    "        for key, value in row.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                row[key] = value.tolist()\n",
    "    # Convert dictionary to JSON\n",
    "    json_result = json.dumps(df_dict, indent=4)\n",
    "    \n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a84ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_distributions(dist_file,  reg=0.2, n_clusters=None, calculate_barycenter=False, stop_theshold=10**-9, num_of_iterations=1000, plt_dendrogram=True):\n",
    "    list_sim_outputs_raw = json_content_to_list_of_lists(dist_file)    \n",
    "    list_base = merge_list(list_sim_outputs_raw)\n",
    "    list_sim_outputs = []\n",
    "    p_list = []\n",
    "    for i in list_sim_outputs_raw:\n",
    "        list_sim_outputs.append(density_calc(i, i)[0])\n",
    "        p_list.append(density_calc(i, i)[1])\n",
    "    \n",
    "    normalized_list_sim_outputs = normalize_tuples(list_sim_outputs)\n",
    "    m = len(normalized_list_sim_outputs)\n",
    "    blank_df = create_blank_dataset_with_metadata(m)\n",
    "    df = fill_dataset_with_records(blank_df, make_record(normalized_list_sim_outputs, p_list))\n",
    "    # Display the filled dataset\n",
    "    df['data points real'] = list_sim_outputs\n",
    "    fill_ot_distance(df, num_of_iterations, reg, stop_theshold)\n",
    "    if plt_dendrogram:\n",
    "        plot_dendrogram(df, save_file=False) \n",
    "    sil_values = silhouette_score_agglomerative(df)\n",
    "#     print(df['47'][0])\n",
    "    if n_clusters==None:# if cluster_num is none then max_silhouette index will be considered, else the number that is wanted\n",
    "        n_clusters = sil_values.index(max(sil_values)) + 2\n",
    "    \n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    diagonal = np.diagonal(matrix)\n",
    "    matrix_final = matrix + matrix.transpose() \n",
    "    np.fill_diagonal(matrix_final, 0)\n",
    "    # Below we get the linkage matrix, which will be used in many parts\n",
    "    upper_triangle_flat = matrix_final[np.triu_indices_from(matrix_final, k=1)]    \n",
    "    Z = hierarchy.linkage(upper_triangle_flat, method='complete') \n",
    "    clusters = hierarchy.fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    df['cluster'] = clusters\n",
    "    print(clusters.tolist())\n",
    "    \n",
    "    # Here the dataset for clusters is generated\n",
    "    blank_df_clusters = create_blank_dataset_with_metadata(n_clusters)\n",
    "    records_to_be_added =[]\n",
    "    for i in range(1,n_clusters+1):\n",
    "        records_to_be_added.append({'cluster num': i, 'p':0})\n",
    "    df_clusters = fill_dataset_with_records(blank_df_clusters, records_to_be_added)    \n",
    "    \n",
    "    \n",
    "    list_p_cluster_new = []\n",
    "    list_sup_cluster_new = []\n",
    "    list_sup_cluster_real_new = []\n",
    "    for i in range(1, len(df_clusters)+1):\n",
    "        df_test  = df[df['cluster']==i]\n",
    "        list_column = df_test['data points']\n",
    "        list_sim_outputs_cluster = list_column.tolist()\n",
    "        list_base_cluster = merge_list(list_sim_outputs_cluster)\n",
    "\n",
    "        list_column_real = df_test['data points real']\n",
    "        list_sim_outputs_cluster_real = list_column_real.tolist()\n",
    "        list_base_cluster_real = merge_list(list_sim_outputs_cluster_real)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cost_matrix_cluster = distance.cdist(list_base_cluster, list_base_cluster)\n",
    "        density_list_cluster = density_calc_list(list_sim_outputs_cluster, list_base_cluster)\n",
    "        df_test['p'] = density_list_cluster\n",
    "        list_p_cluster_new.append(calc_barycenter(df_test, cost_matrix_cluster, reg))\n",
    "        list_sup_cluster_new.append(list_base_cluster)\n",
    "        list_sup_cluster_real_new.append(list_base_cluster_real)\n",
    "        \n",
    "    df_clusters['p'] = list_p_cluster_new\n",
    "    df_clusters['data points'] = list_sup_cluster_new\n",
    "    df_clusters['data points real'] = list_sup_cluster_real_new\n",
    "    json_inputs =  dataframe_to_json(df, ['system num', 'data points real', 'cluster'])\n",
    "    json_barycenters = dataframe_to_json(df_clusters,['data points real', 'p'])\n",
    "\n",
    "    \n",
    "    return json_inputs, json_barycenters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0bb476e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agglomerative_times = []\n",
    "for i in range(1, 11):\n",
    "    filename = f'../../data/tuples_list_{i}.txt'\n",
    "    print(f\"Processing {filename}\")\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        # Read lines from the file and parse tuples of floats\n",
    "        list_sim_outputs_raw = [eval(line.strip()) for line in f]\n",
    "\n",
    "    json_result = list_of_lists_to_json(list_sim_outputs_raw)\n",
    "    \n",
    "    min_time = time.time()\n",
    "    cluster_distributions(json_result, reg=0.2, n_clusters=4, stop_theshold=10**-9, num_of_iterations=1000)\n",
    "    max_time = time.time()\n",
    "    agglomerative_times.append(max_time - min_time)\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffda92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
