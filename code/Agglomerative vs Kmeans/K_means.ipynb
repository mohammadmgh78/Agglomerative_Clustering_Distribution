{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7def70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "import time\n",
    "import json\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ced432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_list(list_of_list):\n",
    "    list_final = []\n",
    "    for i in list_of_list:\n",
    "        list_final += i    \n",
    "    return list(set(list_final))\n",
    "\n",
    "\n",
    "def density_calc(list_base, list_point):\n",
    "    n = len(list_point)\n",
    "    list_base = list(set(list_base))\n",
    "    list_density =[]\n",
    "    list_point_final = []\n",
    "    for i in list_base:\n",
    "        if i in list_point:\n",
    "            list_density.append(list_point.count(i)/n)\n",
    "            list_point_final.append(i)\n",
    "        else:\n",
    "            list_density.append(0) #changed\n",
    "    return  list_point_final, list_density\n",
    "\n",
    "\n",
    "\n",
    "def density_calc_list(list_of_list, list_base):\n",
    "    list_density = []\n",
    "    for i in list_of_list:\n",
    "        list_density.append(np.array([density_calc(list_base, i)[1]]).transpose())\n",
    "    return list_density\n",
    "\n",
    "\n",
    "def create_blank_dataset_with_metadata(m):\n",
    "    data = {\n",
    "        'system num': [],\n",
    "        'data points': [],\n",
    "    }\n",
    "\n",
    "\n",
    "    data[f'label'] = []\n",
    "    blank_dataset = pd.DataFrame(data)\n",
    "    \n",
    "    return blank_dataset\n",
    "\n",
    "\n",
    "def fill_dataset_with_records(dataset, records):\n",
    "    for record in records:\n",
    "        dataset = pd.concat([dataset, pd.DataFrame([record])], ignore_index=True)\n",
    "    return dataset\n",
    "\n",
    "def make_record(list_of_list, list_p):\n",
    "    records_to_be_added = []\n",
    "    for i in range(len(list_of_list)):\n",
    "        records_to_be_added.append({'system num': i, 'data points': list_of_list[i], 'p':list_p[i]})\n",
    "        \n",
    "    return records_to_be_added\n",
    "\n",
    "            \n",
    "def condensed_creator(arr):\n",
    "    m = arr.shape[0]\n",
    "\n",
    "    # Extract upper triangle indices\n",
    "    upper_triangle_indices = np.triu_indices(m, k=1)\n",
    "\n",
    "    # Use the indices to get the upper triangle elements\n",
    "    upper_triangle_elements = arr[upper_triangle_indices]\n",
    "\n",
    "    # Convert the elements to a list if needed\n",
    "    upper_triangle_list = upper_triangle_elements.tolist()\n",
    "\n",
    "    # Print or use the resulting list as needed\n",
    "    return upper_triangle_list\n",
    "\n",
    "def plot_dendrogram(df, save_file=False):\n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    matrix_final = matrix + matrix.transpose()\n",
    "    \n",
    "    scaled_matrix = matrix_final\n",
    "    np.fill_diagonal(scaled_matrix, 0)\n",
    "    \n",
    "    matrix_final = condensed_creator(scaled_matrix)\n",
    "    linkage_matrix = linkage(matrix_final, method='complete')\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(linkage_matrix, color_threshold=-np.inf, above_threshold_color='gray')\n",
    "    plt.xlabel('Systems')  # Set the x-axis label to 'System'\n",
    "    plt.xticks([])  # Remove x-axis tick labels\n",
    "    plt.ylabel('Distance')\n",
    "    if save_file:\n",
    "        plt.savefig('../../results/plot_dendrogram.png', format='png', dpi=1000)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def silhouette_score_agglomerative(df):\n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    matrix_final = matrix + matrix.transpose()\n",
    "    min_val = np.min(matrix)\n",
    "    max_val = np.max(matrix)\n",
    "    scaled_matrix = (matrix_final - min_val)\n",
    "    np.fill_diagonal(scaled_matrix, 0)\n",
    "    silhouette_score_list = []\n",
    "    for i in range(2, len(df)):\n",
    "        index_list = cluster_list_creator(df, i)\n",
    "        silhouette_score_list.append(silhouette_score(scaled_matrix, index_list, metric='precomputed'))\n",
    "    return silhouette_score_list\n",
    "\n",
    "def entropy(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    non_zero_entries = matrix[matrix > 0]\n",
    "    entropy_value = -np.sum(non_zero_entries * np.log(non_zero_entries))\n",
    "\n",
    "    return entropy_value\n",
    "\n",
    "def cluster_list_creator(df, num_of_clusters):\n",
    "    \n",
    "    columns_to_filter = [str(i) for i in range(len(df))]\n",
    "    df_filter = df[columns_to_filter]\n",
    "    filled_df = df_filter.fillna(0)\n",
    "    matrix = filled_df.values\n",
    "    matrix_final = matrix + matrix.transpose()\n",
    "    \n",
    "    min_val = np.min(matrix)\n",
    "    max_val = np.max(matrix)\n",
    "    scaled_matrix = matrix_final\n",
    "    np.fill_diagonal(scaled_matrix, 0)\n",
    "    matrix_final = condensed_creator(scaled_matrix)\n",
    "\n",
    "    linkage_matrix = linkage(matrix_final, method='complete')\n",
    "    \n",
    "    \n",
    "    height = np.shape(linkage_matrix)[0]\n",
    "    list_linkage = [[i] for i in range(len(df))]\n",
    "    for i in range(height):\n",
    "        list_linkage.append(list_linkage[int(linkage_matrix[i][0])] + list_linkage[int(linkage_matrix[i][1])])\n",
    "        \n",
    "        \n",
    "    \n",
    "    list_linkage_inverse = list_linkage[::-1]\n",
    "    list_final = list_linkage_inverse[num_of_clusters-1:]\n",
    "    list_index = []\n",
    "    for i in range(len(df)):\n",
    "        for j in list_final:\n",
    "            if i in j:\n",
    "                list_index.append(list_final.index(j))\n",
    "                break\n",
    "\n",
    "    return list_index\n",
    "\n",
    "def calculate_OT_cost(p, q, reg, cost_matrix, num_iterations, stop_theshold):\n",
    "    p = np.array([p]).T\n",
    "    q = np.array([q]).T\n",
    "    Xi = np.exp(-cost_matrix / reg)\n",
    "    v_n = np.ones((Xi.shape[1], 1))\n",
    "    v_old = v_n\n",
    "    for _ in range(num_iterations):\n",
    "        v_n = q / (Xi.T @ (p / (Xi @ v_n)))\n",
    "        if np.linalg.norm(v_n  - v_old)<stop_theshold:\n",
    "            break\n",
    "        v_old = v_n\n",
    "    diag_u = np.diagflat((p / (Xi @ v_n)))\n",
    "    diag_v = np.diagflat(v_n)\n",
    "    OT_plan = diag_u @ Xi @ diag_v\n",
    "    OT_cost = np.multiply(OT_plan, cost_matrix).sum()\n",
    "    return OT_plan\n",
    "\n",
    "\n",
    "def fill_ot_distance(df, num_of_iterations, lambda_pen, stop_theshold):\n",
    "    for i in range(len(df)):# Here we iterate among rows, and below we shall calculate the densities\n",
    "        for j in range(i+1):\n",
    "\n",
    "            cost_matrix = distance.cdist(df['data points'][i], df['data points'][j])\n",
    "            min_time = time.time()\n",
    "            OT_plan_test = calculate_OT_cost(df['p'][i], df['p'][j], lambda_pen, cost_matrix, num_of_iterations, stop_theshold)            \n",
    "            OT_cost_test = np.multiply(OT_plan_test, cost_matrix).sum()  #yakhoda\n",
    "            max_time = time.time()\n",
    "            df[str(i)][j] = OT_cost_test\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def normalize_tuples(list_of_lists):\n",
    "    num_dimensions = len(list_of_lists[0][0])  # Get the number of dimensions from the first tuple\n",
    "    \n",
    "    # Extract all values for each dimension\n",
    "    all_values = [[] for _ in range(num_dimensions)]\n",
    "    for sublist in list_of_lists:\n",
    "        for i, t in enumerate(sublist):\n",
    "            for j in range(num_dimensions):\n",
    "                all_values[j].append(t[j])\n",
    "    \n",
    "    # Compute the minimum and maximum values for each dimension\n",
    "    min_values = [min(dim_values) for dim_values in all_values]\n",
    "    max_values = [max(dim_values) for dim_values in all_values]\n",
    "    \n",
    "    # Normalize each dimension of each tuple\n",
    "    normalized_list_of_lists = []\n",
    "    for sublist in list_of_lists:\n",
    "        normalized_sublist = []\n",
    "        for t in sublist:\n",
    "            normalized_t = tuple((t[j] - min_values[j]) / (max_values[j] - min_values[j]) for j in range(num_dimensions))\n",
    "            normalized_sublist.append(normalized_t)\n",
    "        normalized_list_of_lists.append(normalized_sublist)\n",
    "    \n",
    "    return normalized_list_of_lists\n",
    "\n",
    "\n",
    "def list_of_lists_to_json(list_of_lists):\n",
    "    json_data = {}\n",
    "\n",
    "    for idx, lst in enumerate(list_of_lists):\n",
    "        json_data[idx + 1] = {\n",
    "            'id': idx + 1,\n",
    "            'data points': lst\n",
    "        }\n",
    "    \n",
    "    json_output = json.dumps(json_data, indent=4)\n",
    "    \n",
    "    return json_output\n",
    "def json_content_to_list_of_lists(json_content):\n",
    "    json_data = json.loads(json_content)\n",
    "    \n",
    "    list_of_lists = []\n",
    "    for key in sorted(json_data.keys(), key=int):\n",
    "        list_of_lists.append([tuple(item) for item in json_data[key]['data points']])\n",
    "    \n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c09e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_array(*lst):\n",
    "    r\"\"\" Convert a list if in numpy format \"\"\"\n",
    "    lst_not_empty = [a for a in lst if len(a) > 0 and not isinstance(a, list)]\n",
    "\n",
    "    if len(lst_not_empty) == 0:\n",
    "        type_as = np.zeros(0)\n",
    "    else:\n",
    "        type_as = lst_not_empty[0]\n",
    "    if len(lst) > 1:\n",
    "        return [np.from_numpay(np.array(a), type_as=type_as)\n",
    "                if isinstance(a, list) else a for a in lst]\n",
    "    else:\n",
    "        if isinstance(lst[0], list):\n",
    "            return np.from_numpy(np.array(lst[0]), type_as=type_as)\n",
    "        else:\n",
    "            return lst[0]\n",
    "        \n",
    "        \n",
    "def geometricBar(weights, alldistribT):\n",
    "    \"\"\"return the weighted geometric mean of distributions\"\"\"\n",
    "    weights, alldistribT = list_to_array(weights, alldistribT)\n",
    "    assert (len(weights) == alldistribT.shape[1])\n",
    "    return np.exp(np.dot(np.log(alldistribT), weights.T))\n",
    "\n",
    "\n",
    "def geometricMean(alldistribT):\n",
    "    \"\"\"return the  geometric mean of distributions\"\"\"\n",
    "    alldistribT = list_to_array(alldistribT)\n",
    "    return np.exp(np.mean(np.log(alldistribT), axis=1))\n",
    "\n",
    "\n",
    "def barycenter_sinkhorn(A, M, reg, weights=None, numItermax=1000,\n",
    "                        stopThr=1e-4, verbose=False, log=False, warn=True):\n",
    "    A, M = list_to_array(A, M)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones((A.shape[1],), dtype=A.dtype) / A.shape[1]\n",
    "    else:\n",
    "        assert (len(weights) == A.shape[1])\n",
    "\n",
    "    if log:\n",
    "        log_dict = {'err': []}\n",
    "\n",
    "    K = np.exp(-M / reg)\n",
    "\n",
    "    err = 1\n",
    "\n",
    "    UKv = np.dot(K, (A.T / np.sum(K, axis=0)).T)\n",
    "    u = (geometricMean(UKv) / UKv.T).T\n",
    "    for ii in range(numItermax):\n",
    "\n",
    "        v =  (A / np.dot(K, u))\n",
    "        ps = np.tile(geometricBar(weights, UKv), (A.shape[1], 1)).T\n",
    "        UKv = u * np.dot(K.T, v)\n",
    "        u = ps/np.dot(K, v)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "        if ii % 10 == 1:\n",
    "            err = np.sum(np.std(UKv, axis=1))\n",
    "\n",
    "            if log:\n",
    "                log_dict['err'].append(err)\n",
    "\n",
    "            if err < stopThr:\n",
    "                break\n",
    "            if verbose:\n",
    "                if ii % 200 == 0:\n",
    "                    print('{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n",
    "                print('{:5d}|{:8e}|'.format(ii, err))\n",
    "    else:\n",
    "        if warn:\n",
    "            warnings.warn(\"Sinkhorn did not converge. You might want to \"\n",
    "                          \"increase the number of iterations `numItermax` \"\n",
    "                          \"or the regularization parameter `reg`.\")\n",
    "    if log:\n",
    "        log_dict['niter'] = ii\n",
    "        return geometricBar(weights, UKv), log_dict\n",
    "    else:\n",
    "        return geometricBar(weights, UKv)\n",
    "    \n",
    "    \n",
    "def calc_barycenter(df, cost_matrix, lambda_value):\n",
    "    list_p_cluster =  [i for i in df['p']]\n",
    "    matrix_p = np.column_stack(list_p_cluster)\n",
    "    return barycenter_sinkhorn(matrix_p, cost_matrix, lambda_value, weights=None, numItermax=200, stopThr=0.0001, verbose=False, log=False, warn=True)\n",
    "\n",
    "\n",
    "def dataframe_to_json(df, columns=None):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a JSON format where each row index maps to a dictionary \n",
    "    of column names and their corresponding values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON string representing the DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to dictionary format\n",
    "    if columns==None:\n",
    "        df_dict = df.to_dict(orient='index')\n",
    "    else:\n",
    "        df_dict = df[columns].to_dict(orient='index')\n",
    "    for row in df_dict.values():\n",
    "        for key, value in row.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                row[key] = value.tolist()\n",
    "    # Convert dictionary to JSON\n",
    "    json_result = json.dumps(df_dict, indent=4)\n",
    "    \n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9368d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_mapping(a, gradient, t0_beta):\n",
    "    # Proximal mapping using Kullback-Leibler divergence as the Bregman divergence\n",
    "\n",
    "\n",
    "    a_tilde = a * np.exp(-t0_beta * gradient)\n",
    "    a_tilde /= np.sum(a_tilde)\n",
    "    return a_tilde\n",
    "\n",
    "\n",
    "\n",
    "def opt_a(X, Y_list, b_list, t0, tol=1e-9, max_iter=1000):\n",
    "    n = X.shape[0]\n",
    "    N = len(Y_list)\n",
    "    \n",
    "    # Form all n x mi matrices Mi\n",
    "    M_list = [np.linalg.norm(X[:, np.newaxis] - Y, axis=2) for Y in Y_list]\n",
    "#     Initialize a_hat and a_tilde\n",
    "    a_hat  = (np.ones(n) / n).reshape((n, 1))\n",
    "    a_tilde = a_hat\n",
    "    t = 3\n",
    "    converged = False\n",
    "    while not converged and t < max_iter:\n",
    "        beta = (t + 1) / 2\n",
    "        a = (1 - beta**-1) * a_hat + beta**-1 * a_tilde\n",
    "        # Form subgradient alpha\n",
    "        alpha_list = [calculate_OT_cost(a, b_list[i], 0.2,M_list[i], num_iterations=100, stop_theshold=10**-9)[1] for i in range(len(b_list))]\n",
    "        alpha = np.mean(-np.log(alpha_list)*0.2, axis=0)\n",
    "        # Update a_tilde using the proximal mapping\n",
    "        t0_beta = t0 * beta\n",
    "        a_tilde = proximal_mapping(a, alpha, t0_beta)\n",
    "#         # Update a_hat\n",
    "        a_hat = (1 - beta**-1) * a_hat + (beta**-1) * a_tilde\n",
    "        \n",
    "#         # Check convergence\n",
    "        if np.linalg.norm(a_tilde - a_hat) < tol:\n",
    "            converged = True\n",
    "        \n",
    "        t += 1\n",
    "    return a_hat\n",
    "\n",
    "\n",
    "def find_barycenter(X, Y_list, b_list, t0, theta, tol=1e-9, max_iter=1000):\n",
    "    iter_num = 1\n",
    "    while iter_num< max_iter:\n",
    "        n = X.shape[0]\n",
    "        N = len(Y_list)\n",
    "        M_list = [np.linalg.norm(X[:, np.newaxis] - Y, axis=2) for Y in Y_list]\n",
    "        \n",
    "        \n",
    "        a_update = opt_a(X, Y_list, b_list, t0, tol=1e-2, max_iter=30)[0]\n",
    "        T_list = [calculate_OT_cost(a_update, b_list[i], 0.2, M_list[i], num_iterations=50, stop_theshold=10**-4)[0] for i in range(len(b_list))]\n",
    "        YT_list = [T_list[i] @ Y_list[i] for i in range(len(Y_list))]\n",
    "\n",
    "        YT_ave = np.mean(YT_list, axis=0)\n",
    "        X_old = X\n",
    "        X = (1-theta) * X + theta * (np.diag((a_update**-1).T[0]) @ YT_ave)\n",
    "\n",
    "        if np.linalg.norm(X - X_old) < tol:\n",
    "            print('BROKE!'*5)\n",
    "            return X, a_update\n",
    "        iter_num += 1 \n",
    "\n",
    "    return X, a_update\n",
    "\n",
    "def calculate_OT_cost(p, q, reg, cost_matrix, num_iterations, stop_theshold):\n",
    "\n",
    "    p = np.array([p]).T\n",
    "    q = np.array([q]).T\n",
    "\n",
    "    Xi = np.exp(-cost_matrix / reg)\n",
    "    v_n = np.ones((Xi.shape[1], 1))\n",
    "    v_old = v_n\n",
    "    for _ in range(num_iterations):\n",
    "        v_n = q / (Xi.T @ (p / (Xi @ v_n)))\n",
    "        if np.linalg.norm(v_n  - v_old)<stop_theshold:\n",
    "            break\n",
    "        v_old = v_n\n",
    "    diag_u = np.diagflat((p / (Xi @ v_n)))\n",
    "    diag_v = np.diagflat(v_n)\n",
    "    OT_plan = diag_u @ Xi @ diag_v\n",
    "    OT_cost = np.multiply(OT_plan, cost_matrix).sum()\n",
    "    return OT_plan, p / (Xi @ v_n), v_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500bedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ot_distance(df, df_clusters, num_of_iterations, lambda_pen, stop_theshold):\n",
    "#     print('entered fill_ot_distance')\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df_clusters)):\n",
    "            cost_matrix = distance.cdist(df['data points'][i], df_clusters['data points'][j])\n",
    "            OT_plan_test = calculate_OT_cost(df['p'][i], df_clusters['p'][j], lambda_pen, cost_matrix, num_of_iterations, stop_theshold)[0]   \n",
    "            \n",
    "            cost_matrix1 = distance.cdist(df['data points'][i], df['data points'][i])\n",
    "            OT_plan_test1 = calculate_OT_cost(df['p'][i], df['p'][i], lambda_pen, cost_matrix1, num_of_iterations, stop_theshold)[0]   \n",
    "   \n",
    "            cost_matrix2 = distance.cdist(df_clusters['data points'][j], df_clusters['data points'][j])\n",
    "            OT_plan_test2 = calculate_OT_cost(df_clusters['p'][j], df_clusters['p'][j], lambda_pen, cost_matrix1, num_of_iterations, stop_theshold)[0]   \n",
    "            \n",
    "            OT_cost_test1 = np.multiply(OT_plan_test1, cost_matrix1).sum() \n",
    "            OT_cost_test2 = np.multiply(OT_plan_test2, cost_matrix2).sum()\n",
    "            \n",
    "            OT_cost_test = np.multiply(OT_plan_test, cost_matrix).sum() - 0.5*(OT_cost_test1 + OT_cost_test2)   #yakhoda\n",
    "#             OT_cost_test = np.multiply(OT_plan_test, cost_matrix).sum()\n",
    "            df[str(j)][i] = OT_cost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0049ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tuples(list_of_lists):\n",
    "    num_dimensions = 2  # Get the number of dimensions from the first tuple\n",
    "    \n",
    "    # Extract all values for each dimension\n",
    "    all_values = [[] for _ in range(num_dimensions)]\n",
    "    for sublist in list_of_lists:\n",
    "        for i, t in enumerate(sublist):\n",
    "            for j in range(num_dimensions):\n",
    "                all_values[j].append(t[j])\n",
    "    \n",
    "    # Compute the minimum and maximum values for each dimension\n",
    "    min_values = [min(dim_values) for dim_values in all_values]\n",
    "    max_values = [max(dim_values) for dim_values in all_values]\n",
    "#     print(min_values)\n",
    "#     print(max_values)\n",
    "    # Normalize each dimension of each tuple\n",
    "    normalized_list_of_lists = []\n",
    "    for sublist in list_of_lists:\n",
    "        normalized_sublist = []\n",
    "        for t in sublist:\n",
    "            normalized_t = tuple((t[j] - min_values[j]) / (max_values[j] - min_values[j]) for j in range(num_dimensions))\n",
    "            normalized_sublist.append(normalized_t)\n",
    "        normalized_list_of_lists.append(normalized_sublist)\n",
    "    \n",
    "    return normalized_list_of_lists, np.array(min_values), np.array(max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/tuples_list.txt', 'r') as f:\n",
    "    # Read lines from the file and parse tuples of floats\n",
    "    list_sim_outputs_raw = [eval(line.strip()) for line in f]\n",
    "\n",
    "json_result = list_of_lists_to_json(list_sim_outputs_raw)\n",
    "# min_time = time.time()\n",
    "# for m in range(2, 20):\n",
    "#     cluster_distributions_kmeans(json_result, reg=0.2, n_clusters=m, stop_theshold=10**-9, num_of_iterations=1000)\n",
    "# max_time = time.time()\n",
    "# print(max_time - min_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c55724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(matrix, min_values, max_values):\n",
    "#     matrix = np.array(matrix)\n",
    "#     matrix = matrix * (max_values - min_values) + min_values\n",
    "#     matrix = [tuple(row) for row in matrix]\n",
    "    return matrix * (max_values - min_values) + min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9620a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_distributions_kmeans(dist_file,  reg=0.2, n_clusters=2, stop_theshold=10**-9, num_of_iterations=100, plt_dendrogram=True):\n",
    "\n",
    "\n",
    "    list_sim_outputs_raw = json_content_to_list_of_lists(dist_file) \n",
    "    list_base = merge_list(list_sim_outputs_raw)\n",
    "    list_sim_outputs = []\n",
    "    p_list = []\n",
    "    for i in list_sim_outputs_raw:\n",
    "        list_sim_outputs.append(density_calc(i, i)[0])\n",
    "        p_list.append(density_calc(i, i)[1])\n",
    "    print(len(list_sim_outputs_raw))\n",
    "    normalized_list_sim_outputs = normalize_tuples(list_sim_outputs)[0]\n",
    "    min_values_global = normalize_tuples(list_sim_outputs)[1]\n",
    "    max_values_global = normalize_tuples(list_sim_outputs)[2]\n",
    "\n",
    "    m = len(normalized_list_sim_outputs)\n",
    "    blank_df = create_blank_dataset_with_metadata(m)\n",
    "    df = fill_dataset_with_records(blank_df, make_record(normalized_list_sim_outputs, p_list))\n",
    "    # Display the filled dataset\n",
    "    df['data points real'] = list_sim_outputs\n",
    "\n",
    "    #Define initial centroids from the instances\n",
    "    initial_clusters = random.sample(range(len(list_sim_outputs_raw)), n_clusters)\n",
    "    print(f'initial_clusters is {initial_clusters}')\n",
    "    #make the centroids dataset\n",
    "    blank_df_clusters = create_blank_dataset_with_metadata(n_clusters)\n",
    "    records_to_be_added =[]\n",
    "    for i in initial_clusters:\n",
    "        records_to_be_added.append({'cluster num': initial_clusters.index(i), 'p':df['p'][i], 'data points':df['data points'][i]})\n",
    "    df_clusters = fill_dataset_with_records(blank_df_clusters, records_to_be_added)  \n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        df[f'{i}'] = [0 for i in range(len(list_sim_outputs_raw))]\n",
    "    fill_ot_distance(df, df_clusters, num_of_iterations, reg, stop_theshold)\n",
    "\n",
    "    columns_to_consider = [str(i) for i in range(n_clusters)]\n",
    "    df['label'] = df[columns_to_consider].idxmin(axis=1)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    cluster_itration = 5\n",
    "    X = np.random.rand(20, 2)\n",
    "\n",
    "    for _ in range(cluster_itration):\n",
    "        min_values_all = []\n",
    "        max_values_all = []\n",
    "        list_bary_X = []\n",
    "        list_bary_prob = []\n",
    "        list_inputs_cluster = []\n",
    "        list_p_cluster_new = []\n",
    "        list_sup_cluster_new = []\n",
    "        list_sup_cluster_real_new = []\n",
    "\n",
    "        for i in range(len(df_clusters)):\n",
    " \n",
    "            df_test  = df[df['label']==i]\n",
    "            if len(df_test) == 1:\n",
    "\n",
    "                df_clusters['data points'][i] = df_test['data points'].tolist()[0]\n",
    "                df_clusters['p'][i] = df_test['p'].tolist()[0]\n",
    "\n",
    "            else:\n",
    "                list_column = df_test['data points']\n",
    "\n",
    "                list_sim_outputs_cluster = list_column.tolist()\n",
    "\n",
    "\n",
    "\n",
    "                list_sim_outputs_cluster =  list_sim_outputs_cluster\n",
    "\n",
    "                list_base_cluster = merge_list(list_sim_outputs_cluster)\n",
    "                list_of_arrays = [np.array(inner_list) for inner_list in list_sim_outputs_cluster]\n",
    "\n",
    "\n",
    "                b_list = [(np.ones(20) / 20 ).tolist() for _ in range(len(list_sim_outputs_cluster))]\n",
    "                t0 = 0.1\n",
    "                theta = 0.1\n",
    "                reg = 0.2\n",
    "                bary_X, bary_a = find_barycenter(X, list_of_arrays, b_list, t0, theta, tol=1e-6, max_iter=50)\n",
    "                bary_X = [tuple(row) for row in bary_X]\n",
    "\n",
    "\n",
    "\n",
    "                df_clusters['data points'][i] = bary_X\n",
    "                df_clusters['p'][i] = bary_a.T.tolist()[0]\n",
    "        fill_ot_distance(df, df_clusters, num_of_iterations, reg, stop_theshold)\n",
    "        columns_to_consider = [str(i) for i in range(n_clusters)]\n",
    "        old_label = df['label'].tolist()\n",
    "        list_indices = df[columns_to_consider].idxmin(axis=1).astype(int).tolist()\n",
    "#         print(list(set(list_indices)))\n",
    "        if len(list(set(list_indices))) < n_clusters:\n",
    "            print('ONE IS MISSING')\n",
    "            return df\n",
    "            missing_indice =[]\n",
    "            for k in range(n_clusters):\n",
    "                if k not in list(set(list_indices)):\n",
    "                    missing_indice.append(k)\n",
    "            for k in missing_indice:\n",
    "                \n",
    "                list_indices[k] = k # you could make it variable\n",
    "            \n",
    "#         df['label'] = df[columns_to_consider].idxmin(axis=1).astype(int).tolist()\n",
    "        df['label'] = list_indices\n",
    "#         print(df[columns_to_consider].idxmin(axis=1).astype(int).tolist())\n",
    "#         df['label'] = df['label'].astype(int) \n",
    "        print(df['label'].tolist())\n",
    "        #what if one of the labels is gone?\n",
    "#         if len(df['label'].tolist())\n",
    "        if old_label == df['label'].tolist():\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e4f576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_distributions_kmeans(dist_file,  reg=0.2, n_clusters=2, stop_theshold=10**-9, num_of_iterations=100, plt_dendrogram=True):\n",
    "\n",
    "\n",
    "    list_sim_outputs_raw = json_content_to_list_of_lists(dist_file) \n",
    "    support_size = len(list_sim_outputs_raw[0])\n",
    "    list_base = merge_list(list_sim_outputs_raw)\n",
    "    list_sim_outputs = []\n",
    "    p_list = []\n",
    "    for i in list_sim_outputs_raw:\n",
    "        list_sim_outputs.append(density_calc(i, i)[0])\n",
    "        p_list.append(density_calc(i, i)[1])\n",
    "    normalized_list_sim_outputs = normalize_tuples(list_sim_outputs)[0]\n",
    "    min_values_global = normalize_tuples(list_sim_outputs)[1]\n",
    "    max_values_global = normalize_tuples(list_sim_outputs)[2]\n",
    "\n",
    "    m = len(normalized_list_sim_outputs)\n",
    "    blank_df = create_blank_dataset_with_metadata(m)\n",
    "    df = fill_dataset_with_records(blank_df, make_record(normalized_list_sim_outputs, p_list))\n",
    "    # Display the filled dataset\n",
    "    df['data points real'] = list_sim_outputs\n",
    "\n",
    "    #Define initial centroids from the instances\n",
    "    initial_clusters = random.sample(range(len(list_sim_outputs_raw)), n_clusters)\n",
    "    #make the centroids dataset\n",
    "    blank_df_clusters = create_blank_dataset_with_metadata(n_clusters)\n",
    "\n",
    "    records_to_be_added =[]\n",
    "    for i in initial_clusters:\n",
    "        records_to_be_added.append({'cluster num': initial_clusters.index(i), 'p':df['p'][i], 'data points':df['data points'][i]})\n",
    "    df_clusters = fill_dataset_with_records(blank_df_clusters, records_to_be_added)  \n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        df[f'{i}'] = [0 for i in range(len(list_sim_outputs_raw))]\n",
    "    fill_ot_distance(df, df_clusters, num_of_iterations, reg, stop_theshold)\n",
    "\n",
    "    columns_to_consider = [str(i) for i in range(n_clusters)]\n",
    "    df['label'] = df[columns_to_consider].idxmin(axis=1)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    list_indices = df[columns_to_consider].idxmin(axis=1).astype(int).tolist()\n",
    "    if len(list(set(list_indices))) < n_clusters:\n",
    "        return df\n",
    "    cluster_itration = support_size\n",
    "    X = np.random.rand(support_size, 2)\n",
    "\n",
    "    for _ in range(cluster_itration):\n",
    "        min_values_all = []\n",
    "        max_values_all = []\n",
    "        list_bary_X = []\n",
    "        list_bary_prob = []\n",
    "        list_inputs_cluster = []\n",
    "        list_p_cluster_new = []\n",
    "        list_sup_cluster_new = []\n",
    "        list_sup_cluster_real_new = []\n",
    "\n",
    "        for i in range(len(df_clusters)):\n",
    "\n",
    "            df_test  = df[df['label']==i]\n",
    "            if len(df_test) == 1:\n",
    "\n",
    "                df_clusters['data points'][i] = df_test['data points'].tolist()[0]\n",
    "                df_clusters['p'][i] = df_test['p'].tolist()[0]\n",
    "\n",
    "            else:\n",
    "                list_column = df_test['data points real']\n",
    "\n",
    "                list_sim_outputs_cluster = list_column.tolist()\n",
    "\n",
    "\n",
    "                min_values_all= normalize_tuples(list_sim_outputs_cluster)[1]\n",
    "                max_values_all= normalize_tuples(list_sim_outputs_cluster)[2]\n",
    "                list_sim_outputs_cluster =  normalize_tuples(list_sim_outputs_cluster)[0]\n",
    "\n",
    "                list_base_cluster = merge_list(list_sim_outputs_cluster)\n",
    "                list_of_arrays = [np.array(inner_list) for inner_list in list_sim_outputs_cluster]\n",
    "\n",
    "\n",
    "                b_list = [(np.ones(support_size) / support_size ).tolist() for _ in range(len(list_sim_outputs_cluster))]\n",
    "                t0 = 0.01\n",
    "                theta = 0.05\n",
    "                reg = 0.2\n",
    "                bary_X, bary_a = find_barycenter(X, list_of_arrays, b_list, t0, theta, tol=1e-6, max_iter=200)\n",
    "                bary_X = [tuple(row) for row in bary_X]\n",
    "                bary_X = bary_X * (max_values_all - min_values_all) + min_values_all \n",
    "                list_bary_X.append(bary_X)\n",
    "                list_bary_prob.append(bary_a.T.tolist()[0])\n",
    "\n",
    "                df_clusters['data points'][i] = (bary_X - min_values_global) / (max_values_global - min_values_global)\n",
    "                df_clusters['p'][i] = bary_a.T.tolist()[0]\n",
    "        fill_ot_distance(df, df_clusters, num_of_iterations, reg, stop_theshold)\n",
    "        columns_to_consider = [str(i) for i in range(n_clusters)]\n",
    "        old_label = df['label'].tolist()\n",
    "        list_indices = df[columns_to_consider].idxmin(axis=1).astype(int).tolist()\n",
    "\n",
    "        if len(list(set(list_indices))) < n_clusters:\n",
    "\n",
    "            return df\n",
    "            \n",
    "\n",
    "        df['label'] = list_indices\n",
    "\n",
    "        if old_label == df['label'].tolist():\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6f4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/tuples_list.txt', 'r') as f:\n",
    "    # Read lines from the file and parse tuples of floats\n",
    "    list_sim_outputs_raw = [eval(line.strip()) for line in f]\n",
    "\n",
    "json_result = list_of_lists_to_json(list_sim_outputs_raw)\n",
    "\n",
    "min_time = time.time()\n",
    "cluster_distributions_kmeans(json_result, reg=0.2, n_clusters=4, stop_theshold=10**-9, num_of_iterations=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/tuples_list.txt', 'r') as f:\n",
    "    # Read lines from the file and parse tuples of floats\n",
    "    list_sim_outputs_raw = [eval(line.strip()) for line in f]\n",
    "\n",
    "json_result = list_of_lists_to_json(list_sim_outputs_raw)\n",
    "min_time = time.time()\n",
    "for m in range(2, 10):\n",
    "    cluster_distributions_kmeans(json_result, reg=0.2, n_clusters=m, stop_theshold=10**-9, num_of_iterations=500)\n",
    "max_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982c770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans_time = []\n",
    "for i in range(1, 11):\n",
    "    filename = f'../../data/tuples_list_{i}.txt'\n",
    "    print(f\"Processing {filename}\")\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        # Read lines from the file and parse tuples of floats\n",
    "        list_sim_outputs_raw = [eval(line.strip()) for line in f]\n",
    "\n",
    "    json_result = list_of_lists_to_json(list_sim_outputs_raw)\n",
    "    \n",
    "    min_time = time.time()\n",
    "    for m in range(2, 10):\n",
    "        cluster_distributions_kmeans(json_result, reg=0.5, n_clusters=m, stop_theshold=10**-9, num_of_iterations=500)\n",
    "    max_time = time.time()\n",
    "    kmeans_time.append(max_time - min_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a165ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=0.2\n",
    "n_clusters=2 \n",
    "calculate_barycenter=False \n",
    "stop_theshold=10**-9\n",
    "num_of_iterations=1000\n",
    "\n",
    "\n",
    "\n",
    "json_result = list_of_lists_to_json(list_sim_outputs_raw)\n",
    "list_sim_outputs_raw = json_content_to_list_of_lists(json_result)\n",
    "list_base = merge_list(list_sim_outputs_raw)\n",
    "list_sim_outputs = []\n",
    "p_list = []\n",
    "for i in list_sim_outputs_raw:\n",
    "    list_sim_outputs.append(density_calc(i, i)[0])\n",
    "    p_list.append(density_calc(i, i)[1])\n",
    "print(len(list_sim_outputs_raw))\n",
    "normalized_list_sim_outputs = normalize_tuples(list_sim_outputs)[0]\n",
    "min_values_global = normalize_tuples(list_sim_outputs)[1]\n",
    "max_values_global = normalize_tuples(list_sim_outputs)[2]\n",
    "\n",
    "\n",
    "\n",
    "m = len(normalized_list_sim_outputs)\n",
    "blank_df = create_blank_dataset_with_metadata(m)\n",
    "df = fill_dataset_with_records(blank_df, make_record(normalized_list_sim_outputs, p_list))\n",
    "# Display the filled dataset\n",
    "df['data points real'] = list_sim_outputs\n",
    "\n",
    "#     print(df)\n",
    "\n",
    "#Define initial centroids from the instances\n",
    "initial_clusters = random.sample(range(len(list_sim_outputs_raw)), n_clusters)\n",
    "\n",
    "\n",
    "#make the centroids dataset\n",
    "blank_df_clusters = create_blank_dataset_with_metadata(n_clusters)\n",
    "records_to_be_added =[]\n",
    "for i in initial_clusters:\n",
    "#         print(i)\n",
    "    records_to_be_added.append({'cluster num': initial_clusters.index(i), 'p':df['p'][i], 'data points':df['data points'][i]})\n",
    "df_clusters = fill_dataset_with_records(blank_df_clusters, records_to_be_added)  \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    df[f'{i}'] = [0 for i in range(len(list_sim_outputs_raw))]\n",
    "fill_ot_distance(df, df_clusters, num_of_iterations, reg, stop_theshold)\n",
    "\n",
    "columns_to_consider = [str(i) for i in range(n_clusters)]\n",
    "df['label'] = df[columns_to_consider].idxmin(axis=1)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "#     print(df) \n",
    "#     print(f\" labe is \\n{df['label'].tolist()}\")\n",
    "cluster_itration = 20\n",
    "X = np.random.rand(10, 2)\n",
    "\n",
    "for _ in range(cluster_itration):\n",
    "    min_values_all = []\n",
    "    max_values_all = []\n",
    "    list_bary_X = []\n",
    "    list_bary_prob = []\n",
    "    list_inputs_cluster = []\n",
    "    list_p_cluster_new = []\n",
    "    list_sup_cluster_new = []\n",
    "    list_sup_cluster_real_new = []\n",
    "\n",
    "    for i in range(len(df_clusters)):\n",
    "        print(f'i is {i}')\n",
    "        df_test  = df[df['label']==i]\n",
    "        if len(df_test) == 1:\n",
    "\n",
    "            df_clusters['data points'][i] = df_test['data points'].tolist()[0]\n",
    "            df_clusters['p'][i] = df_test['p'].tolist()[0]\n",
    "            \n",
    "        else:\n",
    "            list_column = df_test['data points real']\n",
    "\n",
    "            list_sim_outputs_cluster = list_column.tolist()\n",
    "\n",
    "\n",
    "            min_values_all= normalize_tuples(list_sim_outputs_cluster)[1]\n",
    "            max_values_all= normalize_tuples(list_sim_outputs_cluster)[2]\n",
    "            list_sim_outputs_cluster =  normalize_tuples(list_sim_outputs_cluster)[0]\n",
    "\n",
    "            list_base_cluster = merge_list(list_sim_outputs_cluster)\n",
    "            list_of_arrays = [np.array(inner_list) for inner_list in list_sim_outputs_cluster]\n",
    "\n",
    "            \n",
    "            b_list = [(np.ones(10) / 10 ).tolist() for _ in range(len(list_sim_outputs_cluster))]\n",
    "            t0 = 0.005\n",
    "            theta = 0.005\n",
    "            reg = 0.2\n",
    "\n",
    "            bary_X, bary_a = find_barycenter(X, list_of_arrays, b_list, t0, theta, tol=1e-10, max_iter=300)\n",
    "            bary_X = [tuple(row) for row in bary_X]\n",
    "\n",
    "            bary_X = bary_X * (max_values_all - min_values_all) + min_values_all \n",
    "            list_bary_X.append(bary_X)\n",
    "            list_bary_prob.append(bary_a.T.tolist()[0])\n",
    "\n",
    "            df_clusters['data points'][i] = (bary_X - min_values_global) / (max_values_global - min_values_global)\n",
    "            df_clusters['p'][i] = bary_a.T.tolist()[0]\n",
    "\n",
    "    fill_ot_distance(df, df_clusters, num_of_iterations, reg, stop_theshold)\n",
    "\n",
    "    columns_to_consider = [str(i) for i in range(n_clusters)]\n",
    "    old_label = df['label'].tolist()\n",
    "    df['label'] = df[columns_to_consider].idxmin(axis=1)\n",
    "    df['label'] = df['label'].astype(int) \n",
    "\n",
    "\n",
    "    if old_label == df['label'].tolist():\n",
    "        print('DONE')\n",
    "        print('*'*50)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5d992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
